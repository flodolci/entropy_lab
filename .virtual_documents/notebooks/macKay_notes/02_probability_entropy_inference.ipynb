








import math

def shannon_information(probability, base=2):
    return -math.log(probability)/math.log(base)

shannon_information(0.0008)





from entropy_lab.measures.shannon import shannon_information

shannon_information(0.0008)





import pandas as pd

df = pd.read_csv("/Users/fdolci/projects/entropy_lab/data/ogd106_preise_strom_boerse.csv")
df.columns = ["date", "price"]
df.head()


# Some data cleaning
df = df.copy()
df["date"] = pd.to_datetime(df["date"], errors="coerce")
df["price"] = pd.to_numeric(df["price"], errors="coerce")
df = df.dropna(subset=["date", "price"]).sort_values("date")
df.head()


import numpy as np
import matplotlib.pyplot as plt
from entropy_lab.measures.shannon import shannon_information

window = 365          # days 
min_periods = 30    
eps = 1e-6           # avoid p=0 => infinite

prices = df["price"].to_numpy()

p_tail = np.full(len(df), np.nan, dtype=float)

for i in range(len(df)):
    start = max(0, i - window + 1)
    w = prices[start:i+1]
    if len(w) < min_periods:
        continue
    # empirical upper-tail probability: P(price >= today's price)
    p = np.mean(w >= prices[i])
    p_tail[i] = max(p, eps)

df["p_upper_tail"] = p_tail
df["surprisal_bits"] = df["p_upper_tail"].apply(
    lambda p: shannon_information(float(p), base=2.0) if np.isfinite(p) else np.nan
)

fig = plt.figure(figsize=(12, 7), dpi=200)

ax1 = fig.add_subplot(211)
ax1.plot(df["date"], df["price"], linewidth=2.2)
ax1.set_title(f"Price series (last {len(df)} observations)")
ax1.set_ylabel("Price")
ax1.grid(True, alpha=0.22)

ax2 = fig.add_subplot(212, sharex=ax1)
ax2.plot(df["date"], df["surprisal_bits"], linewidth=2.2)
ax2.set_title(f"Surprisal of 'today is unusually high' (rolling window = {window} days)")
ax2.set_ylabel("Surprisal (bits)")
ax2.set_xlabel("Date")
ax2.grid(True, alpha=0.22)

plt.tight_layout()
plt.show()











from entropy_lab.measures.shannon import shannon_information
import numpy as np
import numpy.typing as npt

def compute_entropy(p: npt.NDArray[np.floating], base = 2.0) -> float: 
    h = 0
    for x in p:
        h += x * shannon_information(x, base)
    return h





import numpy as np
from scipy.stats import entropy
from entropy_lab.measures.entropy import compute_entropy

example_array = np.array([0.1, 0.4, 0.5])
compute_entropy(example_array)


import numpy as np
import matplotlib.pyplot as plt

from entropy_lab.measures.entropy import compute_entropy

window = 365
min_periods = 30
bins = 40                 # keep fixed for comparability
eps = 1e-12               # avoid log(0)

prices = df["price"].to_numpy()
H_roll = np.full(len(df), np.nan, dtype=float)

for i in range(len(df)):
    start = max(0, i - window + 1)
    w = prices[start:i+1]
    if len(w) < min_periods:
        continue

    # histogram -> pmf
    counts, _ = np.histogram(w, bins=bins)
    pmf = counts.astype(float) / max(counts.sum(), 1.0)
    pmf = np.clip(pmf, eps, 1.0)          # numerical safety
    pmf = pmf / pmf.sum()

    H_roll[i] = entropy(pmf, base=2.0)

df["entropy_bits"] = H_roll

fig = plt.figure(figsize=(12, 10), dpi=200)

ax1 = fig.add_subplot(311)
ax1.plot(df["date"], df["price"], linewidth=2.2)
ax1.set_title(f"Price series (last {len(df)} observations)")
ax1.set_ylabel("Price")
ax1.grid(True, alpha=0.22)

ax2 = fig.add_subplot(312, sharex=ax1)
ax2.plot(df["date"], df["surprisal_bits"], linewidth=2.2)
ax2.set_title(f"Surprisal of 'today is unusually high' (rolling window = {window} days)")
ax2.set_ylabel("Surprisal (bits)")
ax2.grid(True, alpha=0.22)

ax3 = fig.add_subplot(313, sharex=ax1)
ax3.plot(df["date"], df["entropy_bits"], linewidth=2.2)
ax3.set_title(f"Rolling entropy of the empirical price distribution (bins={bins}, window={window} days)")
ax3.set_ylabel("Entropy (bits)")
ax3.set_xlabel("Date")
ax3.grid(True, alpha=0.22)

plt.tight_layout()
plt.show()












import numpy as np
from entropy_lab.measures.entropy import compute_entropy

# Joint probability distribution example, must sum to 1
p_xy = np.array([
    [0.30, 0.20],
    [0.10, 0.20],
    [0.05, 0.15]
]) 
compute_entropy(p_xy.ravel(), base=2.0, normalize=False)





import numpy as np
from entropy_lab.measures.entropy import compute_joint_entropy

p_xy = np.array([
    [0.30, 0.20],
    [0.10, 0.20],
    [0.05, 0.15]
]) 
compute_joint_entropy(p_xy)














import numpy as np

base = 2
p = np.array([0.5, 0.3, 0.2])
q = np.array([0.4, 0.4, 0.2])

d_pq = np.sum(p * (np.log(p / q) / np.log(base)))
d_pq





from scipy.stats import entropy

entropy(pk=p, qk=q, base=base)





from entropy_lab.measures.entropy import compute_kl_divergence

compute_kl_divergence(p, q, base)



