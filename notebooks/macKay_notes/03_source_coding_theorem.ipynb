{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1324e915",
   "metadata": {},
   "source": [
    "# The Source Coding Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e286b72",
   "metadata": {},
   "source": [
    "## Information content of independent random variables\n",
    "\n",
    "If $x$ and $y$ are independent, the following identities hold\n",
    "$$h\\left(x,y\\right) = h\\left(x\\right) + h\\left(y\\right)$$\n",
    "$$H\\left(X,Y\\right) = H\\left(X\\right) + H\\left(Y\\right)$$\n",
    "I.e., entrop and the SHannon information content are additive for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a83bb2",
   "metadata": {},
   "source": [
    "## Designing informative experiments\n",
    "\n",
    "One important property of the entropy is the following: The entropy of an ensemble $X$ is biggest if all the outcomes have equal probabilitiy $p_i=1/|\\mathcal{A}_X|$. In other words: The outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e05166",
   "metadata": {},
   "source": [
    "### Example: Customer support ticket categories\n",
    "\n",
    "We want to get a better intuition of what this property means. Let us assume that a support team receives tickets in 4 categories:\n",
    "* Billing\n",
    "* Technical issue\n",
    "* Account access\n",
    "* Cancellation\n",
    "\n",
    "If one category happens almost all the time, then each new ticket tells you little (low entropy). If all 4 categories are equally likely, each new ticket is more surprising/informative (high entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a66",
   "metadata": {},
   "source": [
    "Let us start by comparing the entropy of different examples of probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd073bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "labels = [\"Billing\", \"Tech\", \"Access\", \"Cancel\"]\n",
    "\n",
    "distributions = {\n",
    "    \"Very skewed\": np.array([0.85, 0.10, 0.03, 0.02]),\n",
    "    \"Moderately skewed\": np.array([0.55, 0.20, 0.15, 0.10]),\n",
    "    \"Almost balanced\": np.array([0.30, 0.25, 0.25, 0.20]),\n",
    "    \"Uniform (max entropy)\": np.array([0.25, 0.25, 0.25, 0.25])\n",
    "}\n",
    "\n",
    "for name, p in distributions.items():\n",
    "    H = compute_entropy(p)\n",
    "    print(f\"{name:22s} -> H = {H:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5631",
   "metadata": {},
   "source": [
    "As expected, we can observe that:\n",
    "1. Entropy increases as the distribution becomes more balanced.\n",
    "2. The uniform case gives the highest entropy.\n",
    "   \n",
    "We can visualize this property by generating a plot that moves gradually from a peaked distribution to a uniform one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "p_skewed = np.array([0.85, 0.10, 0.03, 0.02])\n",
    "p_uniform = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "alphas = np.linspace(0, 1, 101)\n",
    "entropies = []\n",
    "\n",
    "for a in alphas:\n",
    "    # Interpolate between skewed and uniform\n",
    "    p = (1 - a) * p_skewed + a * p_uniform\n",
    "    H = compute_entropy(p, base=2.0)\n",
    "    entropies.append(H)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(alphas, entropies, linewidth=2)\n",
    "plt.axhline(np.log2(4), linestyle=\"--\", linewidth=1, label=\"Max = log2(4) = 2 bits\")\n",
    "plt.xlabel(\"Balance level (0 = skewed, 1 = uniform)\")\n",
    "plt.ylabel(\"Entropy (bits)\")\n",
    "plt.title(\"Entropy is maximal when outcomes are equally likely\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf45948",
   "metadata": {},
   "source": [
    "### Is the Shannon Information content actually a good measure of information content?\n",
    "\n",
    "Before going further into the field of information theory, we must ask ourself if all these measures we are discussing are actually fitting to describe information. To convince ourselves, we will use an example from the book: The game of submarine? We will try to understand how many bits can one bit convey. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcc2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c164216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
