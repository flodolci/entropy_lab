{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1324e915",
   "metadata": {},
   "source": [
    "# The Source Coding Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e286b72",
   "metadata": {},
   "source": [
    "## Information content of independent random variables\n",
    "\n",
    "If $x$ and $y$ are independent, the following identities hold\n",
    "$$h\\left(x,y\\right) = h\\left(x\\right) + h\\left(y\\right)$$\n",
    "$$H\\left(X,Y\\right) = H\\left(X\\right) + H\\left(Y\\right)$$\n",
    "I.e., entrop and the SHannon information content are additive for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a83bb2",
   "metadata": {},
   "source": [
    "## Designing informative experiments\n",
    "\n",
    "One important property of the entropy is the following: The entropy of an ensemble $X$ is biggest if all the outcomes have equal probabilitiy $p_i=1/|\\mathcal{A}_X|$. In other words: The outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e05166",
   "metadata": {},
   "source": [
    "### Example: Customer support ticket categories\n",
    "\n",
    "We want to get a better intuition of what this property means. Let us assume that a support team receives tickets in 4 categories:\n",
    "* Billing\n",
    "* Technical issue\n",
    "* Account access\n",
    "* Cancellation\n",
    "\n",
    "If one category happens almost all the time, then each new ticket tells you little (low entropy). If all 4 categories are equally likely, each new ticket is more surprising/informative (high entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a66",
   "metadata": {},
   "source": [
    "Let us start by comparing the entropy of different examples of probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd073bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "labels = [\"Billing\", \"Tech\", \"Access\", \"Cancel\"]\n",
    "\n",
    "distributions = {\n",
    "    \"Very skewed\": np.array([0.85, 0.10, 0.03, 0.02]),\n",
    "    \"Moderately skewed\": np.array([0.55, 0.20, 0.15, 0.10]),\n",
    "    \"Almost balanced\": np.array([0.30, 0.25, 0.25, 0.20]),\n",
    "    \"Uniform (max entropy)\": np.array([0.25, 0.25, 0.25, 0.25])\n",
    "}\n",
    "\n",
    "for name, p in distributions.items():\n",
    "    H = compute_entropy(p)\n",
    "    print(f\"{name:22s} -> H = {H:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5631",
   "metadata": {},
   "source": [
    "As expected, we can observe that:\n",
    "1. Entropy increases as the distribution becomes more balanced.\n",
    "2. The uniform case gives the highest entropy.\n",
    "   \n",
    "We can visualize this property by generating a plot that moves gradually from a peaked distribution to a uniform one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "p_skewed = np.array([0.85, 0.10, 0.03, 0.02])\n",
    "p_uniform = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "alphas = np.linspace(0, 1, 101)\n",
    "entropies = []\n",
    "\n",
    "for a in alphas:\n",
    "    # Interpolate between skewed and uniform\n",
    "    p = (1 - a) * p_skewed + a * p_uniform\n",
    "    H = compute_entropy(p, base=2.0)\n",
    "    entropies.append(H)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(alphas, entropies, linewidth=2)\n",
    "plt.axhline(np.log2(4), linestyle=\"--\", linewidth=1, label=\"Max = log2(4) = 2 bits\")\n",
    "plt.xlabel(\"Balance level (0 = skewed, 1 = uniform)\")\n",
    "plt.ylabel(\"Entropy (bits)\")\n",
    "plt.title(\"Entropy is maximal when outcomes are equally likely\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feab4fe",
   "metadata": {},
   "source": [
    "## Data compression\n",
    "\n",
    "We are now convinced that the Shannon information content of an outcome is a natural measure of its information content. In other words, improbable outcomes do convey more information than probable outcomes. Let us now consider how many bits are needed to describe the outcome of an experiment. \n",
    "\n",
    "If we can show that we can compress data from a particular source into a file of $L$ bits per source symbol and recover the data reliably, then we will say that the average information content of that source is at most $L$ bits per symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956b1b2",
   "metadata": {},
   "source": [
    "There are several ways of measuring the information content of a random variable. One simple way is to simply count the number of possible outcomes: $|\\mathcal{A}_X|$. If we give a binary name to each outcome, in the situation where $|\\mathcal{A}_X|$ is a power of 2, the length of each name would be $\\log_2|\\mathcal{A}_X|$. Therefore:\n",
    "\n",
    "**The raw bit content** of $X$:\n",
    "$$H_0\\left(X\\right) = \\log_2|\\mathcal{A}_X|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ecb2d",
   "metadata": {},
   "source": [
    "$H_0\\left(X\\right)$ represents the minimum number of bits needed to uniquely label every possible outcome. Let us look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# alphabet size of a dice: 6\n",
    "dice_alphabet_size = 6\n",
    "raw_bit_content = math.log2(dice_alphabet_size)\n",
    "raw_bit_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197586e",
   "metadata": {},
   "source": [
    "Using the custom-made function in the ```hartley.py``` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy_lab.measures.hartley import raw_bit_content\n",
    "\n",
    "raw_bit_content(dice_alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d42056",
   "metadata": {},
   "source": [
    "Note that this measure of information content does not include any probabilistic element. What it does is simply to map each outcome to a constant-length binary string. This leads us to the following observation:\n",
    "1. A *lossy* compressor compresses some files, but maps some files to the *same* encoding. It may be useful in some situations, such as image compression.\n",
    "2. A *lossless* compressor maps all files to different encodings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b221a",
   "metadata": {},
   "source": [
    "## Information content defined in terms of lossy compression\n",
    "\n",
    "We start our exploration by considering lossy compression. Considering the probability of each symbol in an alphabet (for instance, the English ASCII alphabet in an English text), we can observe that some characters occur more frequently than others. To compress such a text, we could imagine to reduce slightly the size of this alphabet by ignoring the less-frequently present characters, such as \"!\". This means that we would lose the ability to encode some of the more improbable symbols. Therefore, this reduction is associated with a certain *risk*: The larger the risk we are willing to take, the smaller our final alphabet becomes. \n",
    "\n",
    "Let us define a new parameter, $\\delta$, which represents this risk. Formally, $\\delta$ is the probability that there will be no name for an outcome $x$. We can thus define the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f13bb2",
   "metadata": {},
   "source": [
    "**The smallest $\\delta$-sufficient subset** $S_{\\delta}$ is the smallest subset of $\\mathcal{A}_X$ satisfying\n",
    "$$P\\left(x\\in S_{\\delta}\\right) \\geq 1 - \\delta$$\n",
    "\n",
    "We can construct the subset $S_{\\delta}$ by ranking the elements of $\\mathcal{A}_X$ in order of decreasing probability and adding successive elements starting from the most probable elements until the total probability is $\\geq \\left(1-\\delta\\right)$. An implementation of this algorithm, together with a class to define an alphabet, can be found ```coding/alphabet.py```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from entropy_lab.coding.alphabet import AlphabetDistribution\n",
    "\n",
    "text_example = \"\"\"\n",
    "Hello!!! This is an example text... with some rare stuff: @@@ ### $$$\n",
    "Mostly letters and spaces, but a few symbols appear only once.\n",
    "\"\"\"\n",
    "\n",
    "counts = Counter(text_example)\n",
    "counts.pop(\"\\n\", None)\n",
    "counts.pop(\" \", None)\n",
    "\n",
    "dist = AlphabetDistribution.from_counts(dict(counts))\n",
    "\n",
    "print(\"Alphabet size:\", len(dist.symbols))\n",
    "print(\"Top 10 symbols:\", dist.ranked().symbols[:10])\n",
    "\n",
    "delta = 0.05  # allow dropping 5% probability mass\n",
    "res = dist.delta_subset(delta)\n",
    "\n",
    "print(\"\\n--- Î´-sufficient subset result ---\")\n",
    "print(\"delta:\", delta)\n",
    "print(\"threshold (1-delta):\", res.threshold)\n",
    "print(\"kept alphabet size:\", len(res.kept.symbols))\n",
    "print(\"kept_probability:\", res.kept_probability)\n",
    "print(\"dropped_probability:\", res.dropped_probability)\n",
    "\n",
    "print(\"\\nDropped symbols (rare ones):\")\n",
    "print(res.dropped_symbols)\n",
    "\n",
    "print(\"\\nKept distribution (renormalized) top 10:\")\n",
    "kept_ranked = res.kept.ranked()\n",
    "for s, p in zip(kept_ranked.symbols[:10], kept_ranked.p[:10]):\n",
    "    print(repr(s), f\"{p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaad5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
