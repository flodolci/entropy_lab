{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1324e915",
   "metadata": {},
   "source": [
    "# The Source Coding Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e286b72",
   "metadata": {},
   "source": [
    "## Information content of independent random variables\n",
    "\n",
    "If $x$ and $y$ are independent, the following identities hold\n",
    "$$h\\left(x,y\\right) = h\\left(x\\right) + h\\left(y\\right)$$\n",
    "$$H\\left(X,Y\\right) = H\\left(X\\right) + H\\left(Y\\right)$$\n",
    "I.e., entrop and the SHannon information content are additive for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a83bb2",
   "metadata": {},
   "source": [
    "## Designing informative experiments\n",
    "\n",
    "One important property of the entropy is the following: The entropy of an ensemble $X$ is biggest if all the outcomes have equal probabilitiy $p_i=1/|\\mathcal{A}_X|$. In other words: The outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e05166",
   "metadata": {},
   "source": [
    "### Example: Customer support ticket categories\n",
    "\n",
    "We want to get a better intuition of what this property means. Let us assume that a support team receives tickets in 4 categories:\n",
    "* Billing\n",
    "* Technical issue\n",
    "* Account access\n",
    "* Cancellation\n",
    "\n",
    "If one category happens almost all the time, then each new ticket tells you little (low entropy). If all 4 categories are equally likely, each new ticket is more surprising/informative (high entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a66",
   "metadata": {},
   "source": [
    "Let us start by comparing the entropy of different examples of probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd073bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "labels = [\"Billing\", \"Tech\", \"Access\", \"Cancel\"]\n",
    "\n",
    "distributions = {\n",
    "    \"Very skewed\": np.array([0.85, 0.10, 0.03, 0.02]),\n",
    "    \"Moderately skewed\": np.array([0.55, 0.20, 0.15, 0.10]),\n",
    "    \"Almost balanced\": np.array([0.30, 0.25, 0.25, 0.20]),\n",
    "    \"Uniform (max entropy)\": np.array([0.25, 0.25, 0.25, 0.25])\n",
    "}\n",
    "\n",
    "for name, p in distributions.items():\n",
    "    H = compute_entropy(p)\n",
    "    print(f\"{name:22s} -> H = {H:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5631",
   "metadata": {},
   "source": [
    "As expected, we can observe that:\n",
    "1. Entropy increases as the distribution becomes more balanced.\n",
    "2. The uniform case gives the highest entropy.\n",
    "   \n",
    "We can visualize this property by generating a plot that moves gradually from a peaked distribution to a uniform one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "p_skewed = np.array([0.85, 0.10, 0.03, 0.02])\n",
    "p_uniform = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "alphas = np.linspace(0, 1, 101)\n",
    "entropies = []\n",
    "\n",
    "for a in alphas:\n",
    "    # Interpolate between skewed and uniform\n",
    "    p = (1 - a) * p_skewed + a * p_uniform\n",
    "    H = compute_entropy(p, base=2.0)\n",
    "    entropies.append(H)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(alphas, entropies, linewidth=2)\n",
    "plt.axhline(np.log2(4), linestyle=\"--\", linewidth=1, label=\"Max = log2(4) = 2 bits\")\n",
    "plt.xlabel(\"Balance level (0 = skewed, 1 = uniform)\")\n",
    "plt.ylabel(\"Entropy (bits)\")\n",
    "plt.title(\"Entropy is maximal when outcomes are equally likely\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf45948",
   "metadata": {},
   "source": [
    "### Is the Shannon Information content actually a good measure of information content? An example from the Telecom\n",
    "\n",
    "Consider a simplified communication system that produces a stream of events observed by a receiver. We use three possible outcomes:\n",
    "* **D** = normal data event (very common)\n",
    "* **C** = control event (less common)\n",
    "* **R** = retransmission/error-related event (rare)\n",
    "\n",
    "Even though each observed event is just a *symbol*, not all symbols carry the same amount of information. As a recap, according to Shannon, the information content of a specific outcome $x$ is\n",
    "\n",
    "\n",
    "$$h(x)=\\log_2\\left(\\frac{1}{P(x)}\\right)$$\n",
    "\n",
    "So:\n",
    "* common events carry little information,\n",
    "* rare events carry more information.\n",
    "\n",
    "This is also the key intuition behind data compression:\n",
    "frequent symbols should get short codes, rare symbols can have longer codes. Let us simulate all that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from entropy_lab.measures.shannon import shannon_information\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "# Probabilities of telecom events (example)\n",
    "# D = data (common), C = control (less common), R = retransmission/error (rare)\n",
    "symbols = np.array([\"D\", \"C\", \"R\"])\n",
    "probs = np.array([0.90, 0.07, 0.03])\n",
    "\n",
    "# Shannon information content for each symbol\n",
    "info_contents = []\n",
    "for prob in probs:\n",
    "    info_contents.append(shannon_information(prob))\n",
    "\n",
    "for s, p, h in zip(symbols, probs, info_contents):\n",
    "    print(f\"{s}: P={p:.2f}, h(x)={h:.3f} bits\")\n",
    "\n",
    "H = compute_entropy(probs)\n",
    "print(f\"\\nEntropy H(X) = {H:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe2ccf",
   "metadata": {},
   "source": [
    "With the probabilities above:  \n",
    "\n",
    "* **D** is very common, so it carries **little** information.\n",
    "* **C** is less common, so it carries **more** information.\n",
    "* **R** is rare, so it carries **a lot** of information.\n",
    "\n",
    "This is exactly what we expect in telecom logs:\n",
    "\n",
    "* seeing another normal data event is not surprising,\n",
    "* seeing a retransmission/error event is much more surprising and therefore more informative.\n",
    "\n",
    "The entropy $H(X)$ is the average information per observed event.\n",
    "It tells us the ideal average number of bits needed to encode this stream efficiently. Let us now simulate a stream of received events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42) #seed\n",
    "\n",
    "n = 300\n",
    "stream = rng.choice(symbols, size=n, p=probs)\n",
    "\n",
    "# map each symbol to its information content\n",
    "h_map = {s: h for s, h in zip(symbols, info_contents)}\n",
    "h_stream = np.array([h_map[s] for s in stream])\n",
    "\n",
    "# running average information\n",
    "cum_info = np.cumsum(h_stream)\n",
    "running_avg = cum_info / np.arange(1, n+1)\n",
    "\n",
    "print(\"First 20 events:\", \"\".join(stream[:20]))\n",
    "print(\"Final running average:\", running_avg[-1])\n",
    "print(\"Theoretical entropy H:\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6f7b5",
   "metadata": {},
   "source": [
    "Let us now already plot this simulation with a small animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf848964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.arange(1, n + 1)\n",
    "\n",
    "# Pre-create empty scatter-like artists using line objects for each symbol\n",
    "(line_D,) = ax.plot([], [], marker='o', linestyle='', label='D events')\n",
    "(line_C,) = ax.plot([], [], marker='o', linestyle='', label='C events')\n",
    "(line_R,) = ax.plot([], [], marker='o', linestyle='', label='R events')\n",
    "\n",
    "(line_avg,) = ax.plot([], [], linewidth=2, label='Running average information')\n",
    "ax.axhline(H, linestyle='--', linewidth=1.5, label=f'Entropy H(X) = {H:.3f} bits')\n",
    "\n",
    "ax.set_xlim(1, n)\n",
    "ax.set_ylim(0, max(info_contents) + 0.7)\n",
    "ax.set_xlabel(\"Observed event index\")\n",
    "ax.set_ylabel(\"Information (bits)\")\n",
    "ax.set_title(\"Animated telecom event stream: information content over time\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "text_box = ax.text(\n",
    "    0.02, 0.98, \"\",\n",
    "    transform=ax.transAxes,\n",
    "    va=\"top\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85)\n",
    ")\n",
    "\n",
    "# Storage for animated points\n",
    "xD, yD = [], []\n",
    "xC, yC = [], []\n",
    "xR, yR = [], []\n",
    "\n",
    "def update(frame):\n",
    "    i = frame  # 0-based\n",
    "    s = stream[i]\n",
    "    h = h_stream[i]\n",
    "\n",
    "    # Append current point to the correct symbol series\n",
    "    if s == \"D\":\n",
    "        xD.append(i + 1); yD.append(h)\n",
    "    elif s == \"C\":\n",
    "        xC.append(i + 1); yC.append(h)\n",
    "    else:  # \"R\"\n",
    "        xR.append(i + 1); yR.append(h)\n",
    "\n",
    "    # Update symbol point layers\n",
    "    line_D.set_data(xD, yD)\n",
    "    line_C.set_data(xC, yC)\n",
    "    line_R.set_data(xR, yR)\n",
    "\n",
    "    # Update running average line\n",
    "    line_avg.set_data(x[:i+1], running_avg[:i+1])\n",
    "\n",
    "    # Update text\n",
    "    text_box.set_text(\n",
    "        f\"Event {i+1}/{n}\\n\"\n",
    "        f\"Observed symbol: {s}\\n\"\n",
    "        f\"h(x) = {h:.3f} bits\\n\"\n",
    "        f\"Running avg = {running_avg[i]:.3f} bits\\n\"\n",
    "        f\"Entropy H = {H:.3f} bits\"\n",
    "    )\n",
    "\n",
    "    return line_D, line_C, line_R, line_avg, text_box\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=n, interval=80, blit=False)\n",
    "\n",
    "plt.close(fig)  # avoid duplicate static display in notebooks\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd094fc6",
   "metadata": {},
   "source": [
    "Each point is one observed telecom event:\n",
    "\n",
    "* many points are low (the common **D** events),\n",
    "* fewer points are higher (**C** events),\n",
    "* rare points are very high (**R** events).\n",
    "\n",
    "The running average starts unstable, then gradually converges toward the entropy $H(X)$. This is the central intuition:\n",
    "\n",
    "- **Shannon information content** describes the surprise of a single outcome.\n",
    "- **Entropy** is the long-run average surprise of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de773548",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This example also explains why entropy is connected to file size. If we store the event stream in a file, a naive encoding might use the same number of bits for every event. But a better encoding uses the probabilities:\n",
    "\n",
    "- frequent symbol **D** gets a short code,\n",
    "- less frequent **C** gets a longer code,\n",
    "- rare **R** gets the longest code.\n",
    "\n",
    "This reduces the **average number of bits per event**. Shannon's entropy $H(X)$ gives the ideal lower bound (in the average sense) for how many bits per event are needed for lossless compression.\n",
    "\n",
    "So entropy is not just an abstract quantity, but it directly predicts how compressible a random source is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5198b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
