{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1324e915",
   "metadata": {},
   "source": [
    "# The Source Coding Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e286b72",
   "metadata": {},
   "source": [
    "## Information content of independent random variables\n",
    "\n",
    "If $x$ and $y$ are independent, the following identities hold\n",
    "$$h\\left(x,y\\right) = h\\left(x\\right) + h\\left(y\\right)$$\n",
    "$$H\\left(X,Y\\right) = H\\left(X\\right) + H\\left(Y\\right)$$\n",
    "I.e., entrop and the SHannon information content are additive for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a83bb2",
   "metadata": {},
   "source": [
    "## Designing informative experiments\n",
    "\n",
    "One important property of the entropy is the following: The entropy of an ensemble $X$ is biggest if all the outcomes have equal probabilitiy $p_i=1/|\\mathcal{A}_X|$. In other words: The outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e05166",
   "metadata": {},
   "source": [
    "### Example: Customer support ticket categories\n",
    "\n",
    "We want to get a better intuition of what this property means. Let us assume that a support team receives tickets in 4 categories:\n",
    "* Billing\n",
    "* Technical issue\n",
    "* Account access\n",
    "* Cancellation\n",
    "\n",
    "If one category happens almost all the time, then each new ticket tells you little (low entropy). If all 4 categories are equally likely, each new ticket is more surprising/informative (high entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a66",
   "metadata": {},
   "source": [
    "Let us start by comparing the entropy of different examples of probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd073bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "labels = [\"Billing\", \"Tech\", \"Access\", \"Cancel\"]\n",
    "\n",
    "distributions = {\n",
    "    \"Very skewed\": np.array([0.85, 0.10, 0.03, 0.02]),\n",
    "    \"Moderately skewed\": np.array([0.55, 0.20, 0.15, 0.10]),\n",
    "    \"Almost balanced\": np.array([0.30, 0.25, 0.25, 0.20]),\n",
    "    \"Uniform (max entropy)\": np.array([0.25, 0.25, 0.25, 0.25])\n",
    "}\n",
    "\n",
    "for name, p in distributions.items():\n",
    "    H = compute_entropy(p)\n",
    "    print(f\"{name:22s} -> H = {H:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5631",
   "metadata": {},
   "source": [
    "As expected, we can observe that:\n",
    "1. Entropy increases as the distribution becomes more balanced.\n",
    "2. The uniform case gives the highest entropy.\n",
    "   \n",
    "We can visualize this property by generating a plot that moves gradually from a peaked distribution to a uniform one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "p_skewed = np.array([0.85, 0.10, 0.03, 0.02])\n",
    "p_uniform = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "alphas = np.linspace(0, 1, 101)\n",
    "entropies = []\n",
    "\n",
    "for a in alphas:\n",
    "    # Interpolate between skewed and uniform\n",
    "    p = (1 - a) * p_skewed + a * p_uniform\n",
    "    H = compute_entropy(p, base=2.0)\n",
    "    entropies.append(H)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(alphas, entropies, linewidth=2)\n",
    "plt.axhline(np.log2(4), linestyle=\"--\", linewidth=1, label=\"Max = log2(4) = 2 bits\")\n",
    "plt.xlabel(\"Balance level (0 = skewed, 1 = uniform)\")\n",
    "plt.ylabel(\"Entropy (bits)\")\n",
    "plt.title(\"Entropy is maximal when outcomes are equally likely\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feab4fe",
   "metadata": {},
   "source": [
    "## Data compression\n",
    "\n",
    "We are now convinced that the Shannon information content of an outcome is a natural measure of its information content. In other words, improbable outcomes do convey more information than probable outcomes. Let us now consider how many bits are needed to describe the outcome of an experiment. \n",
    "\n",
    "If we can show that we can compress data from a particular source into a file of $L$ bits per source symbol and recover the data reliably, then we will say that the average information content of that source is at most $L$ bits per symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199fc7a",
   "metadata": {},
   "source": [
    "### Example: Compression of IoT status messages\n",
    "\n",
    "To examplify the idea of data compression, let us imagine an IoT sensor sending one status every minute:\n",
    "* ```OK```: very common\n",
    "* ```LOW_BATTERY```: rare\n",
    "* ```SENSOR_ERROR```: very rare\n",
    "* ```CALIBRATION```: occasional\n",
    "* ```MAINTENANCE```: rare\n",
    "\n",
    "We wish to encode these status messages and look for the optimal strategy. Encoding them with the same fixed length code is wasteful, since ```OK``` appears most of the time. Using Shannon's theory we understand that:\n",
    "1. Frequent events carry less information\n",
    "2. Rare events carry more information\n",
    "3. So an efficient code should use:\n",
    "    * short codes for common symbols\n",
    "    * long codes for rare symbols\n",
    "\n",
    "Let us try to simulate and visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e0ca1",
   "metadata": {},
   "source": [
    "**Definition of a simple telemtry source** We model a sensor that sends one status message at a time. Most of the time everything is fine (```OK```), and rare events occur occasionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible status messages and their probabilities\n",
    "symbols = np.array([\n",
    "    \"OK\",\n",
    "    \"CALIBRATION\",\n",
    "    \"LOW_BATTERY\",\n",
    "    \"MAINTENANCE\",\n",
    "    \"SENSOR_ERROR\"\n",
    "])\n",
    "\n",
    "p = np.array([0.85, 0.07, 0.04, 0.03, 0.01], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54a960",
   "metadata": {},
   "source": [
    "**Computation of Shannon information content and entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "from entropy_lab.measures.shannon import shannon_information\n",
    "\n",
    "I = []\n",
    "for proba in p: \n",
    "    I.append(shannon_information(proba))\n",
    "H = compute_entropy(p)\n",
    "\n",
    "print(\"Shannon information content per symbol (bits):\")\n",
    "for s, prob, info in zip(symbols, p, I):\n",
    "    print(f\"{s:15s}  p={prob:.2f}  I(x)={info:.3f} bits\")\n",
    "print(f\"\\nEntropy H(X) = {H:.3f} bits/message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96050e2d",
   "metadata": {},
   "source": [
    "**Comparing with a fixed-length code** Jumping ahead in terms of theory, we can observe that, due to the fact that there is 5 possible messages, a fixed-length binary code would need 3 bits per message (theory comes later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_length_bits = int(np.ceil(np.log2(len(symbols))))\n",
    "print(\"Fixed-length code size:\", fixed_length_bits, \"bits/message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f990185",
   "metadata": {},
   "source": [
    "**Definition of a simple prefix-free variable-length code** Without going into details, let us define an encoding in which: Short code for common events, longer codes for rare ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec716d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prefix-free codebook\n",
    "codebook = {\n",
    "    \"OK\": \"0\",\n",
    "    \"CALIBRATION\": \"100\",\n",
    "    \"LOW_BATTERY\": \"101\",\n",
    "    \"MAINTENANCE\": \"110\",\n",
    "    \"SENSOR_ERROR\": \"111\"\n",
    "}\n",
    "\n",
    "lengths = np.array([len(codebook[s]) for s in symbols], dtype=int)\n",
    "L_avg = np.sum(p * lengths)\n",
    "print(\"Variable-length codebook:\")\n",
    "for s in symbols:\n",
    "    print(f\"{s:15s} -> {codebook[s]} (length {len(codebook[s])})\")\n",
    "print(f\"\\nAverage code length L = {L_avg:.3f} bits/message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e8fca",
   "metadata": {},
   "source": [
    "**Simulation** Now that we have a valid encoding, we can simulate a message stream and comparing the bit usage, in order to make the compression feel more concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Simulate N messages from the source\n",
    "N = 10000\n",
    "samples = rng.choice(symbols, size=N, p=p)\n",
    "\n",
    "# Count occurrences\n",
    "unique, counts = np.unique(samples, return_counts=True)\n",
    "count_dict = dict(zip(unique, counts))\n",
    "\n",
    "# Total bits using fixed-length coding\n",
    "total_bits_fixed = N * fixed_length_bits\n",
    "\n",
    "# Total bits using variable-length coding\n",
    "total_bits_variable = 0\n",
    "for s in symbols:\n",
    "    total_bits_variable += count_dict.get(s, 0) * len(codebook[s])\n",
    "\n",
    "compression_ratio = total_bits_variable / total_bits_fixed\n",
    "savings_percent = (1 - compression_ratio) * 100\n",
    "\n",
    "print(f\"Simulated messages: {N}\")\n",
    "print(f\"Total bits (fixed-length):    {total_bits_fixed}\")\n",
    "print(f\"Total bits (variable-length): {total_bits_variable}\")\n",
    "print(f\"Compression ratio:            {compression_ratio:.3f}\")\n",
    "print(f\"Bit savings:                  {savings_percent:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b278e92",
   "metadata": {},
   "source": [
    "**Visualization** Finally, let us look at the results with the help of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47366d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare values for the final comparison chart\n",
    "labels_metrics = [\"Entropy H(X)\", \"Avg code length L\", \"Fixed-length\"]\n",
    "values_metrics = [H, L_avg, fixed_length_bits]\n",
    "\n",
    "# Sort symbols by probability (looks cleaner and tells the story better)\n",
    "order = np.argsort(p)[::-1]\n",
    "symbols_sorted = symbols[order]\n",
    "p_sorted = p[order]\n",
    "I = np.array(I, dtype=float)\n",
    "I_sorted = I[order]\n",
    "\n",
    "# A cleaner style without forcing a custom color palette\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 11,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"figure.titlesize\": 15\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5.4))\n",
    "\n",
    "# ---------------- Panel 1: Source probabilities ----------------\n",
    "bars0 = axes[0].bar(symbols_sorted, p_sorted, edgecolor=\"black\", linewidth=0.6)\n",
    "axes[0].set_title(\"Source probabilities\")\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].set_ylim(0, max(p_sorted) * 1.18)\n",
    "axes[0].tick_params(axis=\"x\", rotation=35)\n",
    "axes[0].grid(axis=\"y\", alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "# Value labels\n",
    "for rect, val in zip(bars0, p_sorted):\n",
    "    axes[0].text(\n",
    "        rect.get_x() + rect.get_width() / 2,\n",
    "        rect.get_height() + 0.01,\n",
    "        f\"{val:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# ---------------- Panel 2: Self-information ----------------\n",
    "bars1 = axes[1].bar(symbols_sorted, I_sorted, edgecolor=\"black\", linewidth=0.6)\n",
    "axes[1].set_title(\"Shannon information per message\")\n",
    "axes[1].set_ylabel(\"Bits\")\n",
    "axes[1].set_ylim(0, max(I_sorted) * 1.18)\n",
    "axes[1].tick_params(axis=\"x\", rotation=35)\n",
    "axes[1].grid(axis=\"y\", alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "# Value labels\n",
    "for rect, val in zip(bars1, I_sorted):\n",
    "    axes[1].text(\n",
    "        rect.get_x() + rect.get_width() / 2,\n",
    "        rect.get_height() + 0.06,\n",
    "        f\"{val:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# ---------------- Panel 3: Compression comparison ----------------\n",
    "bars2 = axes[2].bar(labels_metrics, values_metrics, edgecolor=\"black\", linewidth=0.6)\n",
    "axes[2].set_title(\"Compression benchmark\")\n",
    "axes[2].set_ylabel(\"Bits per message\")\n",
    "axes[2].set_ylim(0, max(values_metrics + [H + 1]) * 1.2)\n",
    "axes[2].tick_params(axis=\"x\", rotation=15)\n",
    "axes[2].grid(axis=\"y\", alpha=0.25, linestyle=\"--\")\n",
    "\n",
    "# Annotate bars with values\n",
    "for rect, v in zip(bars2, values_metrics):\n",
    "    axes[2].text(\n",
    "        rect.get_x() + rect.get_width() / 2,\n",
    "        rect.get_height() + 0.04,\n",
    "        f\"{v:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Shannon bound (cleaner placement)\n",
    "axes[2].axhline(H, linestyle=\"--\", linewidth=1)\n",
    "axes[2].axhline(H + 1, linestyle=\"--\", linewidth=1)\n",
    "axes[2].text(2.35, H, \"  H(X)\", va=\"center\", fontsize=9)\n",
    "axes[2].text(2.35, H + 1, \"  H(X)+1\", va=\"center\", fontsize=9)\n",
    "\n",
    "# Light spine cleanup\n",
    "for ax in axes:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Entropy Labs - IoT Status Message Compression\", y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4f122",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "* The source is highly skewed (mostly ```OK```), so it has low entropy.\n",
    "* A fixed-length code wastes bits by giving every message 3 bits.\n",
    "* A variable-length code exploits the non-uniformity and gets close to entropy.\n",
    "* This is the core link between information theory and compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956b1b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
