{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1324e915",
   "metadata": {},
   "source": [
    "# The Source Coding Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e286b72",
   "metadata": {},
   "source": [
    "## Information content of independent random variables\n",
    "\n",
    "If $x$ and $y$ are independent, the following identities hold\n",
    "$$h\\left(x,y\\right) = h\\left(x\\right) + h\\left(y\\right)$$\n",
    "$$H\\left(X,Y\\right) = H\\left(X\\right) + H\\left(Y\\right)$$\n",
    "I.e., entrop and the SHannon information content are additive for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a83bb2",
   "metadata": {},
   "source": [
    "## Designing informative experiments\n",
    "\n",
    "One important property of the entropy is the following: The entropy of an ensemble $X$ is biggest if all the outcomes have equal probabilitiy $p_i=1/|\\mathcal{A}_X|$. In other words: The outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e05166",
   "metadata": {},
   "source": [
    "### Example: Customer support ticket categories\n",
    "\n",
    "We want to get a better intuition of what this property means. Let us assume that a support team receives tickets in 4 categories:\n",
    "* Billing\n",
    "* Technical issue\n",
    "* Account access\n",
    "* Cancellation\n",
    "\n",
    "If one category happens almost all the time, then each new ticket tells you little (low entropy). If all 4 categories are equally likely, each new ticket is more surprising/informative (high entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d4a66",
   "metadata": {},
   "source": [
    "Let us start by comparing the entropy of different examples of probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd073bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "labels = [\"Billing\", \"Tech\", \"Access\", \"Cancel\"]\n",
    "\n",
    "distributions = {\n",
    "    \"Very skewed\": np.array([0.85, 0.10, 0.03, 0.02]),\n",
    "    \"Moderately skewed\": np.array([0.55, 0.20, 0.15, 0.10]),\n",
    "    \"Almost balanced\": np.array([0.30, 0.25, 0.25, 0.20]),\n",
    "    \"Uniform (max entropy)\": np.array([0.25, 0.25, 0.25, 0.25])\n",
    "}\n",
    "\n",
    "for name, p in distributions.items():\n",
    "    H = compute_entropy(p)\n",
    "    print(f\"{name:22s} -> H = {H:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec5631",
   "metadata": {},
   "source": [
    "As expected, we can observe that:\n",
    "1. Entropy increases as the distribution becomes more balanced.\n",
    "2. The uniform case gives the highest entropy.\n",
    "   \n",
    "We can visualize this property by generating a plot that moves gradually from a peaked distribution to a uniform one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "p_skewed = np.array([0.85, 0.10, 0.03, 0.02])\n",
    "p_uniform = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "alphas = np.linspace(0, 1, 101)\n",
    "entropies = []\n",
    "\n",
    "for a in alphas:\n",
    "    # Interpolate between skewed and uniform\n",
    "    p = (1 - a) * p_skewed + a * p_uniform\n",
    "    H = compute_entropy(p, base=2.0)\n",
    "    entropies.append(H)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "plt.plot(alphas, entropies, linewidth=2)\n",
    "plt.axhline(np.log2(4), linestyle=\"--\", linewidth=1, label=\"Max = log2(4) = 2 bits\")\n",
    "plt.xlabel(\"Balance level (0 = skewed, 1 = uniform)\")\n",
    "plt.ylabel(\"Entropy (bits)\")\n",
    "plt.title(\"Entropy is maximal when outcomes are equally likely\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feab4fe",
   "metadata": {},
   "source": [
    "## Data compression\n",
    "\n",
    "We are now convinced that the Shannon information content of an outcome is a natural measure of its information content. In other words, improbable outcomes do convey more information than probable outcomes. Let us now consider how many bits are needed to describe the outcome of an experiment. \n",
    "\n",
    "If we can show that we can compress data from a particular source into a file of $L$ bits per source symbol and recover the data reliably, then we will say that the average information content of that source is at most $L$ bits per symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956b1b2",
   "metadata": {},
   "source": [
    "There are several ways of measuring the information content of a random variable. One simple way is to simply count the number of possible outcomes: $|\\mathcal{A}_X|$. If we give a binary name to each outcome, in the situation where $|\\mathcal{A}_X|$ is a power of 2, the length of each name would be $\\log_2|\\mathcal{A}_X|$. Therefore:\n",
    "\n",
    "**The raw bit content** of $X$:\n",
    "$$H_0\\left(X\\right) = \\log_2|\\mathcal{A}_X|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ecb2d",
   "metadata": {},
   "source": [
    "$H_0\\left(X\\right)$ represents the minimum number of bits needed to uniquely label every possible outcome. Let us look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27cd16e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# alphabet size of a dice: 6\n",
    "dice_alphabet_size = 6\n",
    "raw_bit_content = math.log2(dice_alphabet_size)\n",
    "raw_bit_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197586e",
   "metadata": {},
   "source": [
    "Using the custom-made function in the ```hartley.py``` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cc3a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from entropy_lab.measures.hartley import raw_bit_content\n",
    "\n",
    "raw_bit_content(dice_alphabet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d42056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
