{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d2eab4",
   "metadata": {},
   "source": [
    "# 2 Probability, Entropy, and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbfb62",
   "metadata": {},
   "source": [
    "## Definition of entropy and related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ec629",
   "metadata": {},
   "source": [
    "**The Shannon information content of an outcome $x$**\n",
    "$$h\\left(x\\right) = \\mathrm{log}_2\\frac{1}{P\\left(x\\right)}$$\n",
    "It is measured in bits. The Shannon information content is a natural measure of the information content of the event $x=a_i$.\n",
    "\n",
    "In Python, this might look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d033db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def shannon_information(probability, base=2):\n",
    "    return -math.log(probability)/math.log(base)\n",
    "\n",
    "shannon_information(0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9322adf",
   "metadata": {},
   "source": [
    "An implementation of this metrics can be found in the module ```shannon.py``` and will be used like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy_lab.measures.shannon import shannon_information\n",
    "\n",
    "shannon_information(0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6c5fe",
   "metadata": {},
   "source": [
    "**How Surprising are Electricity Prices?**\n",
    "\n",
    "To illustrate the power and intuitiveness of the Shannon Information content, we will here do a small data analysis, in which the development on the spot markets for 'day ahead' prices of electricity is computed in light of this measure. \n",
    "\n",
    "We will use the Shannon Information as a suprirse meter for extreme price events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73abf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/fdolci/projects/entropy_lab/data/ogd106_preise_strom_boerse.csv\")\n",
    "df.columns = [\"date\", \"price\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data cleaning\n",
    "df = df.copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date\", \"price\"]).sort_values(\"date\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from entropy_lab.measures.shannon import shannon_information\n",
    "\n",
    "window = 365          # days \n",
    "min_periods = 30    \n",
    "eps = 1e-6           # avoid p=0 => infinite\n",
    "\n",
    "prices = df[\"price\"].to_numpy()\n",
    "\n",
    "p_tail = np.full(len(df), np.nan, dtype=float)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    start = max(0, i - window + 1)\n",
    "    w = prices[start:i+1]\n",
    "    if len(w) < min_periods:\n",
    "        continue\n",
    "    # empirical upper-tail probability: P(price >= today's price)\n",
    "    p = np.mean(w >= prices[i])\n",
    "    p_tail[i] = max(p, eps)\n",
    "\n",
    "df[\"p_upper_tail\"] = p_tail\n",
    "df[\"surprisal_bits\"] = df[\"p_upper_tail\"].apply(\n",
    "    lambda p: shannon_information(float(p), base=2.0) if np.isfinite(p) else np.nan\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7), dpi=200)\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(df[\"date\"], df[\"price\"], linewidth=2.2)\n",
    "ax1.set_title(f\"Price series (last {len(df)} observations)\")\n",
    "ax1.set_ylabel(\"Price\")\n",
    "ax1.grid(True, alpha=0.22)\n",
    "\n",
    "ax2 = fig.add_subplot(212, sharex=ax1)\n",
    "ax2.plot(df[\"date\"], df[\"surprisal_bits\"], linewidth=2.2)\n",
    "ax2.set_title(f\"Surprisal of 'today is unusually high' (rolling window = {window} days)\")\n",
    "ax2.set_ylabel(\"Surprisal (bits)\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "ax2.grid(True, alpha=0.22)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fd3a8",
   "metadata": {},
   "source": [
    "**The entropy of an ensemble $X$** \n",
    "\n",
    "This measure is defined as being the average Shannon information content of an outcome:\n",
    "$$H\\left(X\\right) = \\sum_{x\\in\\mathcal{A}_{X}} P\\left(x\\right) \\log\\frac{1}{P\\left(x\\right)} \\quad \\quad [\\mathrm{bits}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b13da",
   "metadata": {},
   "source": [
    "Another name for the entropy of $X$ is the *uncertainty* of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d79e85",
   "metadata": {},
   "source": [
    "We can compute the entropy of a random variable $X$ using the Shannon Information content, as defined earlier, and we thus can write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e925ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy_lab.measures.shannon import shannon_information\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "def compute_entropy(p: npt.NDArray[np.floating], base = 2.0) -> float: \n",
    "    h = 0\n",
    "    for x in p:\n",
    "        h += x * shannon_information(x, base)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0441d07",
   "metadata": {},
   "source": [
    "We can now use our custom-made entropy function the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a32ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "example_array = np.array([0.1, 0.4, 0.5])\n",
    "compute_entropy(example_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac40c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "window = 365\n",
    "min_periods = 30\n",
    "bins = 40                 # keep fixed for comparability\n",
    "eps = 1e-12               # avoid log(0)\n",
    "\n",
    "prices = df[\"price\"].to_numpy()\n",
    "H_roll = np.full(len(df), np.nan, dtype=float)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    start = max(0, i - window + 1)\n",
    "    w = prices[start:i+1]\n",
    "    if len(w) < min_periods:\n",
    "        continue\n",
    "\n",
    "    # histogram -> pmf\n",
    "    counts, _ = np.histogram(w, bins=bins)\n",
    "    pmf = counts.astype(float) / max(counts.sum(), 1.0)\n",
    "    pmf = np.clip(pmf, eps, 1.0)          # numerical safety\n",
    "    pmf = pmf / pmf.sum()\n",
    "\n",
    "    H_roll[i] = entropy(pmf, base=2.0)\n",
    "\n",
    "df[\"entropy_bits\"] = H_roll\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10), dpi=200)\n",
    "\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.plot(df[\"date\"], df[\"price\"], linewidth=2.2)\n",
    "ax1.set_title(f\"Price series (last {len(df)} observations)\")\n",
    "ax1.set_ylabel(\"Price\")\n",
    "ax1.grid(True, alpha=0.22)\n",
    "\n",
    "ax2 = fig.add_subplot(312, sharex=ax1)\n",
    "ax2.plot(df[\"date\"], df[\"surprisal_bits\"], linewidth=2.2)\n",
    "ax2.set_title(f\"Surprisal of 'today is unusually high' (rolling window = {window} days)\")\n",
    "ax2.set_ylabel(\"Surprisal (bits)\")\n",
    "ax2.grid(True, alpha=0.22)\n",
    "\n",
    "ax3 = fig.add_subplot(313, sharex=ax1)\n",
    "ax3.plot(df[\"date\"], df[\"entropy_bits\"], linewidth=2.2)\n",
    "ax3.set_title(f\"Rolling entropy of the empirical price distribution (bins={bins}, window={window} days)\")\n",
    "ax3.set_ylabel(\"Entropy (bits)\")\n",
    "ax3.set_xlabel(\"Date\")\n",
    "ax3.grid(True, alpha=0.22)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb751e2",
   "metadata": {},
   "source": [
    "**Some properties of the entropy**\n",
    "\n",
    "* $H\\left(X\\right) \\geq 0$ with equality iff $p_i = 1$ for one $i$.\n",
    "* Entropy is maximized if $\\mathbf{p}$ is uniform: $H\\left(X\\right) \\leq \\log\\left(|\\mathcal{A}_X|\\right)$ with equality iff $p_i=1/|\\mathcal{A}_X|$ for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf056373",
   "metadata": {},
   "source": [
    "**The joint entropy of $X, Y$**\n",
    "\n",
    "$$H\\left(X,Y\\right) = \\sum_{xy\\in\\mathcal{A}_X\\mathcal{A}_Y}P\\left(x,y\\right)\\log\\frac{1}{P\\left(x,y\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e71ad",
   "metadata": {},
   "source": [
    "Using Python, one can compute the joint entropy in the folliwng way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from entropy_lab.measures.entropy import compute_entropy\n",
    "\n",
    "# Joint probability distribution example, must sum to 1\n",
    "p_xy = np.array([\n",
    "    [0.30, 0.20],\n",
    "    [0.10, 0.20],\n",
    "    [0.05, 0.15]\n",
    "]) \n",
    "compute_entropy(p_xy.ravel(), base=2.0, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36955e",
   "metadata": {},
   "source": [
    "Using the custome-made function in the ```entropy``` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from entropy_lab.measures.entropy import compute_joint_entropy\n",
    "\n",
    "p_xy = np.array([\n",
    "    [0.30, 0.20],\n",
    "    [0.10, 0.20],\n",
    "    [0.05, 0.15]\n",
    "]) \n",
    "compute_joint_entropy(p_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7fe31",
   "metadata": {},
   "source": [
    "For an illustrative example of the joint entropy (and its link to the information - theory comes later), you can have a look at the experiment notebook ```medical_screening_test.py```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0df726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
