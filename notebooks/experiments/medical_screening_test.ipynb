{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e89262",
   "metadata": {},
   "source": [
    "# Medical Screening Test\n",
    "\n",
    "This notebook examplify the meaning of the joint entropy measure, with the help of a fictional medical screening test. Let us consider the following random variables:\n",
    "* $X$: Actual condition in {Healthy, Positive}\n",
    "* $Y$: Test result in {Negative, Positive}\n",
    "\n",
    "We will vary test quality from weak to strong and observe how the joint entropy $H\\left(X,Y\\right)$ and the shared information $I\\left(X;Y\\right)$ change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc15d8",
   "metadata": {},
   "source": [
    "Let us consider the following parameters: The disease prevalence (actual condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# P(Healthy) = 0.9, P(Positive) = 0.1\n",
    "p_x = np.array([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119852a",
   "metadata": {},
   "source": [
    "We now will need to simulate the generation of a joint probability distribution for this test. The following code can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_joint_distribution(q: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build p(x,y) for a binary medical test.\n",
    "\n",
    "    q in [0,1] controls test quality:\n",
    "        q = 0.0 -> weak test (both sensitivity/specificity = 0.5, near random)\n",
    "        q = 1.0 -> strong test (sensitivity = 0.95, specificity = 0.95)\n",
    "    \n",
    "    X rows: [Healthy, Positive]\n",
    "    Y cols: [Negative, Positive]\n",
    "    \"\"\"\n",
    "    if not (0.0 <= q <= 1.0):\n",
    "        raise ValueError(\"q must be in [0,1].\")\n",
    "    sensitivity = 0.5 + 0.45 * q \n",
    "    specificity = 0.5 + 0.45 * q \n",
    "    # Conditional probabilities:\n",
    "    # For Healthy: P(Neg|Healthy)=specificity, P(Pos|Healthy)=1-specificity\n",
    "    # For Positive: P(Neg|Positive)=1-sensitivity, P(Pos|Positive)=sensitivity\n",
    "    p_xy = np.array([\n",
    "        [p_x[0] * specificity, p_x[0] * (1 - specificity)],\n",
    "        [p_x[1] * (1 - sensitivity), p_x[1] * sensitivity],\n",
    "    ])\n",
    "    return p_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dcb6d4",
   "metadata": {},
   "source": [
    "We will now run our experiment over different qualities of test. To do so, we can create the following interval for the $q$ variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "qs = np.linspace(0.0, 1.0, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe800f0e",
   "metadata": {},
   "source": [
    "We are now ready to run our simple experiment and displaying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c576e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy_lab.measures.entropy import compute_entropy, compute_joint_entropy\n",
    "\n",
    "Hx_vals, Hy_vals, Hxy_vals, Hsum_vals, MI_vals = [], [], [], [], []\n",
    "\n",
    "for q in qs:\n",
    "    p_xy = build_test_joint_distribution(q)\n",
    "\n",
    "    p_x_marg = p_xy.sum(axis=1)  # actual condition\n",
    "    p_y_marg = p_xy.sum(axis=0)  # test result\n",
    "\n",
    "    Hx = compute_entropy(p_x_marg)\n",
    "    Hy = compute_entropy(p_y_marg)\n",
    "    Hxy = compute_joint_entropy(p_xy)\n",
    "    Ixy = Hx + Hy - Hxy\n",
    "\n",
    "    Hx_vals.append(Hx)\n",
    "    Hy_vals.append(Hy)\n",
    "    Hxy_vals.append(Hxy)\n",
    "    Hsum_vals.append(Hx + Hy)\n",
    "    MI_vals.append(Ixy)\n",
    "\n",
    "Hx_vals = np.array(Hx_vals)\n",
    "Hy_vals = np.array(Hy_vals)\n",
    "Hxy_vals = np.array(Hxy_vals)\n",
    "Hsum_vals = np.array(Hsum_vals)\n",
    "MI_vals = np.array(MI_vals)\n",
    "\n",
    "# Show endpoint tables for interpretation\n",
    "p_weak = build_test_joint_distribution(0.0)\n",
    "p_strong = build_test_joint_distribution(1.0)\n",
    "\n",
    "print(\"Weak test (q=0.0):\")\n",
    "print(p_weak)\n",
    "print(\"sum =\", p_weak.sum(), \"\\n\")\n",
    "\n",
    "print(\"Strong test (q=1.0):\")\n",
    "print(p_strong)\n",
    "print(\"sum =\", p_strong.sum(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, figsize=(10, 10), sharex=True\n",
    ")\n",
    "\n",
    "# Top subplot: Entropy curves\n",
    "ax1.plot(qs, Hx_vals, label=\"H(X): Actual condition\")\n",
    "ax1.plot(qs, Hy_vals, label=\"H(Y): Test result\")\n",
    "ax1.plot(qs, Hxy_vals, label=\"H(X,Y): Joint\")\n",
    "ax1.plot(qs, Hsum_vals, \"--\", label=\"H(X)+H(Y)\")\n",
    "ax1.set_ylabel(\"Entropy (bits)\")\n",
    "ax1.set_title(\"Joint Entropy in a Medical Screening System\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Bottom subplot: Shared information\n",
    "ax2.plot(qs, MI_vals, label=\"I(X;Y) = H(X)+H(Y)-H(X,Y)\")\n",
    "ax2.set_xlabel(\"Test quality\")\n",
    "ax2.set_ylabel(\"Shared information (bits)\")\n",
    "ax2.set_title(\"How Informative the Test Is About the True Condition\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189463c",
   "metadata": {},
   "source": [
    "As test quality improves, the test result becomes less random and more aligned with the true condition. This reduces the joint uncertainty $H\\left(X,y\\right)$ and increases the mutual information \n",
    "$I\\left(X;Y\\right)$, i.e., the amount of information the test provides about the patient's actual condition.\n",
    "\n",
    "Interestingly, even with a strong test (95% sensitivity/specificity), the mutual information remains below 0.3 bits because the disease prevalence is low (10%). This shows a key information-theoretic principle: The usefulness of a test depends not only on its accuracy, but also on the base rate of the condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121cf34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entropy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
